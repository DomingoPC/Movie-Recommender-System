{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content-Based Recommender System\n",
    "\n",
    "A recommender system is an algorithm or model that takes in information about a user and suggests an item—new to them—that is likely to be of interest. There are several approaches to building such a system, and this notebook will focus on content-based methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Content-Based Recommender Systems use the metadata of the items and users to give recommendations. In this notebook, the system will recommend movies to watch, so some examples of movie metadata could be movie genres, the name of the director, the production company, popularity, etc. At the same time, users metadata could be use, which include movies previously watched, age and the country they are from.\n",
    "\n",
    "The main advantage of this approach is that in Cold-Start Scenarios—i.e. when there is still not enough user interaction—the system is still capable of giving sensible recommendations. This is due to the fact that metadata is already known—at least for the movies. However, this method does not compute similarity between the users, so the ones with a higher correlation cannot be given a stronger impact in the decision.\n",
    "\n",
    "For the approaches to build a system like this, the options vary depending on the metadata to use or the computation power available.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Jaccard and Cosine Similarities\n",
    "> A simple approach and valid when the metadata can be treated as tags, which is common.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "These two are popular similarity metrics that are useful when comparing metadata of various items. For both of them, the first step is to make a matrix $\\underline{\\underline{M}}$ of pairs (movie, tags), where the elements $m_{ij}$ are booleans indicating if the movie *i* has the tag *j*. For example, let's say that there is a DataFrame with two columns, the first the name of the film and the second one of its genres:\n",
    "\n",
    "<center>\n",
    "\n",
    "| **Movie** | **Genre** |\n",
    "|:---------:|:---------:|\n",
    "|     A     |     x     |\n",
    "|     A     |     y     |\n",
    "|     A     |     z     |\n",
    "|     B     |     x     |\n",
    "|     B     |     g     |\n",
    "\n",
    "</center>\n",
    "\n",
    "To transform it into the $\\underline{\\underline{M}}$ matrix, the movies will have to turn into rows and the genres into columns, like such:\n",
    "\n",
    "<center>\n",
    "\n",
    "| **Movie** \\\\ **Genre** | **x** | **y** | **z** | **g** |\n",
    "|------------------------|-------|-------|-------|-------|\n",
    "| **A**                  | 1     | 1     | 1     | 0     |\n",
    "| **B**                  | 1     | 0     | 0     | 1     |\n",
    "\n",
    "</center>\n",
    "\n",
    "Consequently, the elements turn into indicators that show whether the corresponding movie falls under the listed genres. Finally, this allows the movies to be represented as vector—of the space of genres:\n",
    "\n",
    "$$\\underline{A} = (1,1,1,0) \\;\\text{ and }\\; \\underline{B} = (1,0,0,1)$$\n",
    "\n",
    "Now that the movies are expressed as vector, the similarity measure is quite straightforward, as show the following equations:\n",
    "\n",
    "+ **Jaccard Similarity**:   $J(\\underline{A}, \\underline{B}) = \\frac{\\underline{A}\\cap\\underline{B}}{\\underline{A}\\cup\\underline{B}}$\n",
    "\n",
    "+ **Cosine Similarity**:    $Sim(\\underline{B},\\underline{B}) = \\frac{\\underline{A}\\cdot\\underline{B}}{|\\underline{A}||\\underline{B}|}$\n",
    "\n",
    "Coming back to the previous example, the results would be as follows:\n",
    "+ **Jaccard Similarity**:   $J(\\underline{A}, \\underline{B}) = \\frac{(1,1,1,0)\\cap(1,0,0,1)}{(1,1,1,0)\\cup(1,0,0,1)} = \\frac{1+0+0+1}{1+1+1+1} = \\frac{1}{2}$\n",
    "\n",
    "+ **Cosine Similarity**:    $Sim(\\underline{B},\\underline{B}) = \\frac{(1,1,1,0)\\cdot(1,0,0,1)}{|(1,1,1,0)||(1,0,0,1)|} = \\frac{1+0+0+1}{|\\sqrt{1^2+1^2+1^2}||\\sqrt{1^2+1^2}|} = \\sqrt{\\frac{2}{3}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    ">An appropriate approach when the metadata is text-based, which could enclose additional information not represented in tags. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say there is term *t* in a document *d*, which is part of a collection of documents—or corpus—called *D*. Then the *Term Frequency* or $TF(t,d)$ is defined as the number of times a term *t* appears in a document *d*. This can be used on its own as a way to vectorize documents, but it is very easy to **over-emphasize terms** that appear often yet **carry very little information**, such as the terms 'a', 'the' and 'of'. To counter this issue, every term should be weighted by its 'importance' or its ability to uniquely identify a specific document. \n",
    "\n",
    "To obtain a measure of the importance of every term, firstly it is necessary to define the *Document Frequency* or $DF(t,D)$, which is the number of documents in the corpus *D* that contain the term *t*. According to this, the higher the value of $DF(t,D)$, the lower the importance of *t*, because it doesn't **point to a specific document**. Consequently, a low value in $DF(t,D)$ would indicate that the term *t* carries special information about a particular document, thus the interest is in minimizing this value or alternatively in **maximizing the inverse** of $DF(t,D)$, which can be defined as:\n",
    "\n",
    "$$IDF(t,D) = \\log\\left( \\frac{|D| + 1}{DF(t,D) + 1} \\right),$$\n",
    "\n",
    "where: \n",
    "\n",
    "+ $|D|$ is the total number of documents in the corpus *D*.\n",
    "\n",
    "+ The $+1$ in the denominator is a smoothing factor that avoids dividing by zero, which happens when the term *t* does not appear in any document *D*.\n",
    "\n",
    "+ The $+1$ in the numerator is necessary to avoid computing $\\log(0)$, which can happen if there is no document in the corpus *D*.\n",
    "\n",
    "Note that this equation succeeds in representing the importance of a term *t*, because when a term *t* appears in all documents of *D*, then $|D| = DF(t,D)$, thus $IDF(t,D) = \\log(1) = 0$, whereas the lower $DF(t,D)$ is, the greater the value of $IDF(t,D)$.\n",
    "\n",
    "Finally, the *TF-IDF* measure is calculated following:\n",
    "\n",
    "$$TFIDF(t,d,D) = TF(t,d)\\, IDF(t,D).$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code application\n",
    "In code, the process is similar, if done in a simplistic manner:\n",
    "\n",
    "1. **Tokenization of text:** split sentences into individual words and stored in lists.\n",
    "\n",
    "2. **Compute TF:** using *CountVectorizer* or *HashingTF*—more about them later—, calculate the term frequency for every word and store the resulting list in a new column.\n",
    "\n",
    "3. **Compute IDF:** take the column where the TF is stored—i.e. previous output—and apply IDF to *scale the values* according to term importance.\n",
    "\n",
    "4. **Computing Similarity:** with the vectorize version of the the documents via TF-IDF, similarity can be calculated like in the previous section.\n",
    "\n",
    "However, it is possible to perform additional actions to **improve the quality of the documents**, leading to better results in a more efficient way:\n",
    "\n",
    "1. **Text preprocessing:** cleaning the documents. This includes converting to lowercase, removing punctuation and special characters—so that for example, 'word' and 'word.' are treated equally—, and removing numeric values.\n",
    "\n",
    "2. **Tokenization.**\n",
    "\n",
    "3. **Removing stopwords:** common words, such as 'a', 'the' and 'of', will receive a low IDF score in most cases, so removing them just accelerates the process.\n",
    "\n",
    "4. **Optional, but costly options:**\n",
    "\n",
    "    + **Stemming:** reduces words to their root. E.g. 'running' turns into 'run'.\n",
    "\n",
    "    + **Lemmatization:** finds the dictionary form of words. E.g. 'better' turns into 'good'.\n",
    "\n",
    "5. **Computing TF.**\n",
    "\n",
    "6. **Computing IDF.**\n",
    "\n",
    "7. **Computing Similarity.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### About CountVectorizer and HashingTF\n",
    "\n",
    "Both *CountVetorizer* and *HashingTF* can be used to generate the $TF(t,d)$ values, thus vectorizing the document based on its terms. However, they are fundamentally different approaches, starting with their definitions:\n",
    "\n",
    "+ **CountVectorizer:** It is a feature extraction method that converts text documents into a sparse matrix of term counts. To do this, firstly it *builds a vocabulary* of all unique terms in the corpus *D* and then counts occurrences of each term in every document. This approach is useful when it is necessary to *keep track of words* and their frequencies.\n",
    "\n",
    "+ **HashingTF:** It is a transformation that *maps terms* into a fixed-length feature vector *using a hashing function*. Instead of building a vocabulary, it applies a hash function to each term and assigns it to a predefined number of features, also called 'buckets'. This allows for *efficient computation* in exchange of interpretability, since different terms might hash to the same index (*hash collision*).\n",
    "\n",
    "Moreover, let's discuss some specific differences between the two:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table border=\"1\">\n",
    "    <tr>\n",
    "        <th>Feature</th>\n",
    "        <th>CountVectorizer</th>\n",
    "        <th>HashingTF</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><b>Interpretability</b></td>\n",
    "        <td>Keeps the actual words in the vocabulary, making it easy to understand which words contribute to a document.</td>\n",
    "        <td>The words are hashed, so we lose the ability to interpret which word corresponds to which feature.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><b>Memory Usage</b></td>\n",
    "        <td>Requires storing the vocabulary, which can be large for big datasets.</td>\n",
    "        <td>Uses a fixed-length vector, reducing memory requirements since it does not store the vocabulary.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><b>Computational Overhead</b></td>\n",
    "        <td>Requires an extra step to scan the dataset and build the vocabulary before transforming text into vectors.</td>\n",
    "        <td>Directly transforms text using a hashing function, i.e. it can transform the documents in one pass.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><b>Handling of New Words</b></td>\n",
    "        <td>If a new word appears that was not in the training data, it will be ignored (unless vocabulary expansion is allowed).</td>\n",
    "        <td>Can handle new words dynamically without needing retraining.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><b>Risk of Collisions</b></td>\n",
    "        <td>No risk; each word has a unique index.</td>\n",
    "        <td>Has a small chance of hash collisions where different words map to the same index, potentially reducing accuracy.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><b>Scalability</b></td>\n",
    "        <td>May struggle with very large datasets due to vocabulary size constraints.</td>\n",
    "        <td>More scalable since it does not require storing or updating a vocabulary.</td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end, it is possible to summarize this information focusing on their use cases:\n",
    "\n",
    "+ **CountVectorizer:** recommended if *interpretability* is important and there is enough memory to store the vocabulary.\n",
    "\n",
    "+ **HashingTF:** more optimized for larger datasets, especially when *memory efficiency* is required, and when *new words* are expected frequently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec and LSH\n",
    ">A more advanced way to handle text. It has a NN architecture that can manage synonyms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Word2Vec is based on a Neural Network and is capable of placing similar words closer together in the vector space built from the vocabulary. To do this, it takes into account the surrounding words for context, which favors the synonyms detection. Two of the possible approaches are:\n",
    "\n",
    "+ **CBOW**: the Continuous Bag of Words model can predict words from context. It is optimal for large datasets, as it is fast and efficient, and it performs specially well with frequent words.\n",
    "\n",
    "+ **Skip-gram**: this model can predict context from a target word. It is optimal for small datasets, due to its computational cost, but it is the best fitted to work with rare words.\n",
    "\n",
    "After using one of the two architectures for Word2Vec, it is recommended to follow with LSH and euclidean distance, due to the way Word2Vec returns its output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD\n",
    ">A dimensionality reduction technique to increase efficiency in similarity calculations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Singular Value Decomposition or SVD is a technique that decomposes a matrix into three. For instance, let's say a matrix $\\underline{\\underline{A}}$ is storing information about movies like this:\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "\n",
    "|             | **Feature 1** | **Feature 2** | **Feature 3** |\n",
    "|:-----------:|:-------------:|:-------------:|:-------------:|\n",
    "| **Movie 1** |      0.3      |      0.1      |      0.0      |\n",
    "| **Movie 2** |      0.5      |      0.0      |      0.8      |\n",
    "\n",
    "Table 1: Example of a movie-feature matrix.\n",
    "\n",
    "</center>\n",
    "<br />\n",
    "\n",
    "Where 'Features' refer to any way to identify a movie. Let's be more specific and work with movie genres, although the values will be arbitrary:\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "\n",
    "|             | **Space** | **Time** | **Travel** | **Alien** | **Love** |\n",
    "|:-----------:|:---------:|:--------:|:----------:|:---------:|:--------:|\n",
    "| **Movie 1** |     1     |     1    |      1     |     0     |     0    |\n",
    "| **Movie 2** |     0     |     0    |      1     |     0     |     1    |\n",
    "\n",
    "Table 2: A more specific example of a movie-feature matrix using genres as features.\n",
    "\n",
    "</center>\n",
    "<br />\n",
    "\n",
    "Now that the matrix $\\underline{\\underline{A}}$ is defined, the basic idea of SVD is that instead of representing the movies with these 5 genres—which in turn is a five dimensional space—, they will be represented in fewer, yet more general 'Topics'. These 'Topics' could relate to the original genres as follows:\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "\n",
    "|                      | **Space** | **Time** | **Travel** | **Alien** | **Love** |\n",
    "|:--------------------:|:---------:|:--------:|:----------:|:---------:|:--------:|\n",
    "|  **Topic 1: Sci-Fi** |    0.7    |    0.6   |     0.8    |    0.5    |    0.0   |\n",
    "| **Topic 2: Romance** |    0.0    |    0.1   |     0.5    |    0.3    |    0.9   |\n",
    "\n",
    "Table 3: Representation of Features in the new space.\n",
    "\n",
    "</center>\n",
    "<br />\n",
    "\n",
    "And consequently, allow this new representation:\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "\n",
    "|             | **Topic 1: Sci-Fi** | **Topic 2: Romance** |\n",
    "|:-----------:|:-------------------:|:--------------------:|\n",
    "| **Movie 1** |         0.9         |          0.1         |\n",
    "| **Movie 2** |         0.2         |          0.8         |\n",
    "\n",
    "Table 4: Representation of Movies in the new space.\n",
    "\n",
    "</center>\n",
    "<br />\n",
    "\n",
    "In summary, with SVD it is possible to reduce the dimensionality of the problem at hand, making it less computationally intensive, and thus making the similarity calculation faster.\n",
    "\n",
    "#### SVD with a more mathematical approach\n",
    "\n",
    "Let's say that our matrix $\\underline{\\underline{A}}$ (movies as rows and features as columns) is a $n\\times m$ matrix, meaning that it is representing $n$ movies with $m$ features. To describe the movies in a different way—with more general features, as with the 'Topics' before—, $\\underline{\\underline{A}}$ will be factorized in its *latent factors*.\n",
    "\n",
    "The maximum number of latent factors is determined by $\\underline{\\underline{A}}$ and can be extracted from its *rank*:\n",
    "\n",
    "$$k \\equiv \\text{rank}(\\underline{\\underline{A}}) < \\min(n,m).$$\n",
    "\n",
    "So the factorization of a matrix $\\underline{\\underline{A}}_{n\\times m}$ and rank $k$ in its latent factors would be as follows:\n",
    "\n",
    "$$\\underline{\\underline{A}}_{n\\times m} = \\underline{\\underline{U}}_{n\\times k}\\; \\underline{\\underline{\\Sigma}}_{k\\times k}\\; \\underline{\\underline{V}}_{k\\times m}^T,$$\n",
    "\n",
    "where:\n",
    "\n",
    "+ $\\underline{\\underline{U}}_{n\\times k}$ is the representation of the movies in the new space, i.e. Table 4.\n",
    "\n",
    "+ $\\underline{\\underline{\\Sigma}}_{k\\times k}$ is a diagonal matrix that contains the *singular values* of the original matrix, which indicate the importance of each latent factor.\n",
    "\n",
    "+ $\\underline{\\underline{V}}_{k\\times m}^T$ represents the features in the new space, i.e. Table 3.\n",
    "\n",
    "Finally, using the singular values from the matrix $\\underline{\\underline{\\Sigma}}$ it is possible to determine which latent values are more important to the current data. This opens the possibility of truncation, meaning that there is no need to keep all $k$ latent factors, but only the most important ones to represent the data more efficiently, if only sacrificing accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding a Content-Based Recommender System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://127.0.0.1:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>CBRS</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x23277412a00>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import helper_functions as hf\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import FloatType, StringType\n",
    "\n",
    "# Initialize Spark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"CBRS\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------+-----------------+--------------------+----------+--------------------+--------------------+------------+-------+----------------+--------+--------------------+--------------------+-----+------------+----------+--------+----------------------+----------------------+------------------+\n",
      "|adult|              genres|    id|original_language|            overview|popularity|production_companies|production_countries|release_date|runtime|spoken_languages|  status|             tagline|               title|video|vote_average|vote_count|n_genres|n_production_companies|n_production_countries|n_spoken_languages|\n",
      "+-----+--------------------+------+-----------------+--------------------+----------+--------------------+--------------------+------------+-------+----------------+--------+--------------------+--------------------+-----+------------+----------+--------+----------------------+----------------------+------------------+\n",
      "|false|      [Drama, Music]|277216|               en|In 1987, five you...| 21.183077|[New Line Cinema,...|[United States of...|  2015-08-13|  147.0|       [English]|Released| The Story of N.W.A.|Straight Outta Co...|false|         7.7|      1381|       2|                     6|                     1|                 1|\n",
      "|false|             [Drama]|167284|               en|This movie portra...|  0.153287|                  []|                  []|  2004-01-01|  113.0|              []|Released|                NULL|        Viva Algeria|false|         7.2|         3|       1|                     0|                     0|                 0|\n",
      "|false|[Crime, Music, Ro...|298078|               en|A merciless hit m...|  0.782841|                  []|       [Philippines]|  2014-10-24|   73.0|      [日本語, ]|Released|When violence is ...|Ruined Heart: Ano...|false|         6.7|         7|       3|                     0|                     1|                 2|\n",
      "+-----+--------------------+------+-----------------+--------------------+----------+--------------------+--------------------+------------+-------+----------------+--------+--------------------+--------------------+-----+------------+----------+--------+----------------------+----------------------+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Movies Metadata (Load Dataset)\n",
    "df = spark.read.parquet('data/cleaned/movies2/')\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard and Cosine Similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a simple recommender system by movie genre:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|               title|genre|\n",
      "+--------------------+-----+\n",
      "|Straight Outta Co...|Drama|\n",
      "|Straight Outta Co...|Music|\n",
      "|        Viva Algeria|Drama|\n",
      "+--------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Explode genre list into different columns\n",
    "genres = df.select('genres', 'title')\n",
    "genres = df.select(df.title, f.explode(df.genres).alias('genre'))\n",
    "genres.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+\n",
      "|               title|genre|value|\n",
      "+--------------------+-----+-----+\n",
      "|Straight Outta Co...|Drama|    1|\n",
      "|Straight Outta Co...|Music|    1|\n",
      "|        Viva Algeria|Drama|    1|\n",
      "+--------------------+-----+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a columns of 1 to indicate a movies has a specific genre\n",
    "genres = genres.withColumn('value', f.lit(1))\n",
    "genres.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+---------+---------+------+-----+-----------+-----+------+-------+-------+-------+------+-----+-------+-------+---------------+--------+--------+----+-------+\n",
      "|               title|Action|Adventure|Animation|Comedy|Crime|Documentary|Drama|Family|Fantasy|Foreign|History|Horror|Music|Mystery|Romance|Science Fiction|TV Movie|Thriller| War|Western|\n",
      "+--------------------+------+---------+---------+------+-----+-----------+-----+------+-------+-------+-------+------+-----+-------+-------+---------------+--------+--------+----+-------+\n",
      "|         We Are Many|  NULL|     NULL|     NULL|  NULL| NULL|          1| NULL|  NULL|   NULL|   NULL|   NULL|  NULL| NULL|   NULL|   NULL|           NULL|    NULL|    NULL|NULL|   NULL|\n",
      "|All The Days Befo...|  NULL|     NULL|     NULL|     1| NULL|       NULL|    1|  NULL|   NULL|   NULL|   NULL|  NULL| NULL|   NULL|      1|           NULL|    NULL|    NULL|NULL|   NULL|\n",
      "|      Snowman's Land|  NULL|     NULL|     NULL|     1|    1|       NULL| NULL|  NULL|   NULL|   NULL|   NULL|  NULL| NULL|   NULL|   NULL|           NULL|    NULL|       1|NULL|   NULL|\n",
      "+--------------------+------+---------+---------+------+-----+-----------+-----+------+-------+-------+-------+------+-----+-------+-------+---------------+--------+--------+----+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set movies as rows (groupby) and genres as columns (pivot)\n",
    "# sum() makes sure it is one on the (title, genre) pairs indicated above\n",
    "genres = genres.groupby('title').pivot('genre').sum('value')\n",
    "genres.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+---------+---------+------+-----+-----------+-----+------+-------+-------+-------+------+-----+-------+-------+---------------+--------+--------+---+-------+\n",
      "|               title|Action|Adventure|Animation|Comedy|Crime|Documentary|Drama|Family|Fantasy|Foreign|History|Horror|Music|Mystery|Romance|Science Fiction|TV Movie|Thriller|War|Western|\n",
      "+--------------------+------+---------+---------+------+-----+-----------+-----+------+-------+-------+-------+------+-----+-------+-------+---------------+--------+--------+---+-------+\n",
      "|         We Are Many|     0|        0|        0|     0|    0|          1|    0|     0|      0|      0|      0|     0|    0|      0|      0|              0|       0|       0|  0|      0|\n",
      "|All The Days Befo...|     0|        0|        0|     1|    0|          0|    1|     0|      0|      0|      0|     0|    0|      0|      1|              0|       0|       0|  0|      0|\n",
      "|      Snowman's Land|     0|        0|        0|     1|    1|          0|    0|     0|      0|      0|      0|     0|    0|      0|      0|              0|       0|       1|  0|      0|\n",
      "+--------------------+------+---------+---------+------+-----+-----------+-----+------+-------+-------+-------+------+-----+-------+-------+---------------+--------+--------+---+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Finally, set the NULLs as 0, which indicate that the movie hasn't that genre\n",
    "genres = genres.fillna(0)\n",
    "genres.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This DataFrame stores the movie-genre relationships, which are required to calculate the similarities. In this scenario, it is a good idea to preemptively calculate similarities and cache them, so the recommendations can happen in real time. To do this, the first step is to turn the movies into vectors, which is an easy task considering the rows of the movie-genre DataFrame is a vector representation of the movies in the genre space. However, to save up memory, it is wise to use dense vectors, because most of the values are zeros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+---------------------------+\n",
      "|title                       |features                   |\n",
      "+----------------------------+---------------------------+\n",
      "|We Are Many                 |(20,[5],[1.0])             |\n",
      "|All The Days Before Tomorrow|(20,[3,6,14],[1.0,1.0,1.0])|\n",
      "|Snowman's Land              |(20,[3,4,17],[1.0,1.0,1.0])|\n",
      "+----------------------------+---------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "# Train vector assembler on the dataframe\n",
    "vector_assembler = VectorAssembler(inputCols=genres.columns[1:],\n",
    "                                   outputCol='features')\n",
    "\n",
    "# Create the column features with the Dense Vector representation\n",
    "movie_features = vector_assembler.transform(genres).select('title', 'features')\n",
    "movie_features.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to calculate the similarities for each pair of movies, let's join the resulting DataFrame with itself. This will result into a DataFrame whose rows will contain two movies—thus, two dense vectors. However, there are two things that must be guaranteed:\n",
    "\n",
    "+ For every row, the two movies cannot be the same. This would lead to the calculation of the similarity between them, which will not do any good to the recommender system.\n",
    "\n",
    "+ The pair (A,B) yields the same similarity than the pair (B,A), so there is no need to calculate them both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+--------------------+--------------------+\n",
      "|      title|      features|               title|            features|\n",
      "+-----------+--------------+--------------------+--------------------+\n",
      "|We Are Many|(20,[5],[1.0])|         We Are Many|      (20,[5],[1.0])|\n",
      "|We Are Many|(20,[5],[1.0])|All The Days Befo...|(20,[3,6,14],[1.0...|\n",
      "|We Are Many|(20,[5],[1.0])|      Snowman's Land|(20,[3,4,17],[1.0...|\n",
      "+-----------+--------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cross Join: Cartesian Product of the tables\n",
    "df_cross = movie_features.alias('left').crossJoin(movie_features.alias('right'))\n",
    "df_cross.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To filter out the rows with the same movie twice and the repeated pairs, Python allows the use of '<' to compare text. For example:\n",
    "\n",
    "+ 'The Hunger Games' < 'Braveheart' = False.\n",
    "\n",
    "+ 'Braveheart' < 'The Hunger Games' = True.\n",
    "\n",
    "Because 'B' comes before 'T', due to the alphabetical order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+--------------------+--------------------+\n",
      "|      title|      features|               title|            features|\n",
      "+-----------+--------------+--------------------+--------------------+\n",
      "|We Are Many|(20,[5],[1.0])|   What No One Knows|(20,[6,17],[1.0,1...|\n",
      "|We Are Many|(20,[5],[1.0])|What Have They Do...|(20,[13,17],[1.0,...|\n",
      "|We Are Many|(20,[5],[1.0])|     We are the tide|(20,[6,13,15],[1....|\n",
      "+-----------+--------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cross = df_cross.filter(f.col('left.title') < f.col('right.title'))\n",
    "df_cross.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the similarities will be stored in a new column and calculated via User Defined Functions (udf):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Jaccard Similarity ---\n",
    "def jaccard_similarity(v1, v2):\n",
    "    # Formula:\n",
    "    #   J = Intersection(A,B) / Union(A,B)\n",
    "    A_and_B = sum(1 for (a,b) in zip(v1,v2) if a==1 and a==b) # Intersection\n",
    "    A_or_B = sum(1 for (a,b) in zip(v1,v2) if a==1 or b==1) # Union\n",
    "    return float(A_and_B) / A_or_B if A_or_B != 0 else 0.0 # Similarity\n",
    "\n",
    "# --- Cosine Similarity ---\n",
    "def cosine_similarity(v1,v2):\n",
    "    # Formula:\n",
    "    #   Sim = A·B / |A||B|\n",
    "\n",
    "    # Numerator: scalar product\n",
    "    num = sum(c1*c2 for (c1,c2) in zip(v1,v2))\n",
    "    \n",
    "    # Denominator: modules\n",
    "    mod_a = np.sqrt(sum(c1**2 for c1 in v1))\n",
    "    mod_b = np.sqrt(sum(c2**2 for c2 in v2))\n",
    "    den = mod_a * mod_b\n",
    "\n",
    "    # Similarity\n",
    "    return float(num) / float(den) if den != 0.0 else 0.0\n",
    "\n",
    "# --- User Defined Functions ---\n",
    "jaccard_udf = f.udf(lambda v1,v2: jaccard_similarity(v1,v2), FloatType())\n",
    "cosine_udf = f.udf(lambda v1,v2: cosine_similarity(v1,v2), FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+-------+------+\n",
      "|      title|               title|jaccard|cosine|\n",
      "+-----------+--------------------+-------+------+\n",
      "|We Are Many|   What No One Knows|    0.0|   0.0|\n",
      "|We Are Many|What Have They Do...|    0.0|   0.0|\n",
      "|We Are Many|     We are the tide|    0.0|   0.0|\n",
      "|We Are Many|Window Water Baby...|    1.0|   1.0|\n",
      "|We Are Many|           Wood Job!|    0.0|   0.0|\n",
      "+-----------+--------------------+-------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply udf to the pairs of movies in every row\n",
    "df_similarities = df_cross.\\\n",
    "    withColumn('jaccard', jaccard_udf(f.col('left.features'), f.col('right.features'))).\\\n",
    "    withColumn('cosine', cosine_udf(f.col('left.features'), f.col('right.features'))).\\\n",
    "    select('left.title', 'right.title', 'jaccard', 'cosine')\n",
    "\n",
    "df_similarities.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF and LSH\n",
    "\n",
    "From the dataset, only the *movie_id*, *title* and *overview* will be chosen. The terms in the overviews will yield the TF-IDF vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+--------------------+\n",
      "| id|              title|            overview|\n",
      "+---+-------------------+--------------------+\n",
      "|  2|              Ariel|Taisto Kasurinen ...|\n",
      "|  3|Shadows in Paradise|An episode in the...|\n",
      "|  5|         Four Rooms|It's Ted the Bell...|\n",
      "+---+-------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "overview = df.select('id', 'title', 'overview').orderBy('id').na.drop(subset=[\"overview\"])\n",
    "overview.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's focus on transforming the data to make the process more efficient and vectorize the documents. As it was established before, this should include: preprocessing, tokenization, removing stopwords and computing both TF and IDF. *PySpark* can manage most of these, but for the preprocessing is required a specific behavior, thus the best approach would be to implement a custom stage for the pipeline.\n",
    "\n",
    "To create a custom stage for a pipeline one option is to create an user defined function (udf), but the preprocessing stage is slightly complex, as it must convert the text to lowercase and remove any punctuation and special characters. For this reason, a more elegant solution is to use **custom Transformers**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Transformer\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol\n",
    "import re\n",
    "\n",
    "class TextPreprocessing(Transformer, HasInputCol, HasOutputCol):\n",
    "    '''\n",
    "    A custom Transformer which applies preprocessing to the text: convert to lowercase and remove punctuation and special characters\n",
    "    '''\n",
    "\n",
    "    def __init__(self, input_column, output_column='clean_text'):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define inputCol and inputOut\n",
    "        self.inputCol = input_column\n",
    "        self.outputCol = output_column\n",
    "    \n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        # Define text transformation\n",
    "        def lowercase_and_punctuation(text):\n",
    "            lower = text.lower() # lowercase\n",
    "            return re.sub(pattern=r'[^\\w\\s]', repl='', string=lower) # remove all but words, digits and white spaces\n",
    "\n",
    "        udf_preprocess = f.udf(lambda text: lowercase_and_punctuation(text))\n",
    "\n",
    "        # Apply transformation\n",
    "        return df.withColumn(\n",
    "            self.outputCol, udf_preprocess(f.col('overview'))\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the rest of them, the classes included in PySpark are enough to build a proper Pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StopWordsRemover\n",
    "\n",
    "# Text preprocessing: convert to lowercase and remove punctuation and special characters\n",
    "preprocessor = TextPreprocessing(input_column='overview', output_column='preprocessed')\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer(inputCol=\"preprocessed\", outputCol=\"words\")\n",
    "\n",
    "# Remove Stopwords\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "\n",
    "# TF\n",
    "count_vectorizer = CountVectorizer(\n",
    "    inputCol=\"words\", outputCol=\"rawFeatures\", minDF=2, vocabSize=500\n",
    ")\n",
    "\n",
    "hashing_tf = HashingTF(\n",
    "    inputCol='words', outputCol='rawFeatures', numFeatures=500\n",
    ")\n",
    "\n",
    "# IDF\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "\n",
    "# Build a Pipeline with CountVectorizer and another with HashingTF \n",
    "pipeline_CV = Pipeline(stages=[preprocessor, tokenizer, remover, count_vectorizer, idf])\n",
    "pipeline_HTF = Pipeline(stages=[preprocessor, tokenizer, remover, hashing_tf, idf])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for both CountVectorizer and HashingTF—in the TF stage definition—some limits were imposed:\n",
    "\n",
    "+ Firstly and more evident, there is a maximum number of features, which is actually much lower than the default size. This is a consequence of having a large number of movies, due to the elevate time it takes to evaluate the similarities of just one movie and the rest. To solve this issue, one option would be to make a previous selection of movies with a lighter method of based on cached information, to then apply the TF-IDF method to a smaller sample.\n",
    "\n",
    "+ Secondly, with CountVectorizer exists the possibility of imposing conditions on the terms to be considered. For this matter, one of the most important one that can be done is requiring a term to appear in more than 1 (or more) documents, because if a term only appears in one document, then the similarity calculation of this element will always be zero—except for when a document is compared with itself. Furthermore, having terms that appear only in a few documents could lead to overfitting.\n",
    "\n",
    "Now, let's continue by building the models to transform the text into vectors and applying them to the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get features by applying the pipeline\n",
    "model_CV = pipeline_CV.fit(overview)\n",
    "model_HTF = pipeline_HTF.fit(overview)\n",
    "\n",
    "results_CV = model_CV.transform(overview).select('id', 'title', 'overview', 'features')\n",
    "results_HTF = model_HTF.transform(overview).select('id', 'title', 'overview', 'features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute similarity of a chosen movie with the rest instead of calculating it for every possible combination, as TF-IDF is a costly method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen movie: Pirates of the Caribbean: The Curse of the Black Pearl.\n",
      "Chosen movie: Pirates of the Caribbean: The Curse of the Black Pearl.\n"
     ]
    }
   ],
   "source": [
    "# Create similarity function with a target vector\n",
    "def create_similarity_function(movie_id: int, method: str = 'cosine', results: DataFrame = None):\n",
    "    # Method validation\n",
    "    if method not in ('cosine', 'jaccard'):\n",
    "        print('Error: only \"cosine\" or \"jaccard\" methods available.')\n",
    "        return None\n",
    "    \n",
    "    # Movie selector validation\n",
    "    try:\n",
    "        # Get title and vector representation\n",
    "        movie = results.\\\n",
    "            filter(results.id == movie_id).\\\n",
    "            select('title', 'features').collect()[0]\n",
    "\n",
    "        # Show movie name\n",
    "        print(f'Chosen movie: {movie[0]}.')\n",
    "\n",
    "        # Broadcast (cache) movie vector for quick access\n",
    "        bc_mv = spark.sparkContext.broadcast(movie[1])\n",
    "\n",
    "    except IndexError:\n",
    "        print(f'Error: no movie with the index {movie_id} was found.')\n",
    "        return None\n",
    "\n",
    "    # Chose method to compute similarity\n",
    "    if method == 'cosine':\n",
    "        def sim_computation(other_movie):\n",
    "            # Formula: sim = A·B /|A||B|\n",
    "            num = float(np.dot(bc_mv.value, other_movie))\n",
    "            \n",
    "            mod_a = float(np.sqrt(np.dot(bc_mv.value, bc_mv.value)))\n",
    "            mod_b = float(np.sqrt(np.dot(other_movie, other_movie)))\n",
    "            den = mod_a * mod_b\n",
    "\n",
    "            # If either A or B is an empty string, there is\n",
    "            # nothing to compare, thus return no similarity (0)\n",
    "            return num / den if den != 0 else 0.0\n",
    "    \n",
    "    else: # jaccard similarity\n",
    "        def sim_computation(other_movie):\n",
    "            # Formula: Sim = intersection(A,B) / Union(A,B)\n",
    "            A_and_B = float(sum(1 for (a,b) in zip(bc_mv.value,other_movie) if a==1 and a==b)) # Intersection\n",
    "            A_or_B = float(sum(1 for (a,b) in zip(bc_mv.value,other_movie) if a==1 or b==1)) # Union\n",
    "            \n",
    "            # If either A or B is an empty string, there is\n",
    "            # nothing to compare, thus return no similarity (0)\n",
    "            return A_and_B / A_or_B if A_or_B != 0 else 0.0 # Similarity\n",
    "\n",
    "    return f.udf(lambda vec: sim_computation(vec), FloatType())\n",
    "\n",
    "# Create similarity functions\n",
    "example_movie_id = 22\n",
    "\n",
    "udf_similiarity_CV = create_similarity_function(\n",
    "    movie_id=example_movie_id, method='cosine', results=results_CV # <-- Specify movie id to compare\n",
    ")\n",
    "\n",
    "udf_similiarity_HTF = create_similarity_function(\n",
    "    movie_id=example_movie_id, method='cosine', results=results_HTF # <-- Specify movie id to compare\n",
    ")\n",
    "\n",
    "# Compute Similarities\n",
    "similarities_CV = results_CV.withColumn(\n",
    "    'similarity', udf_similiarity_CV('features')\n",
    ").\\\n",
    "    select('id', 'title', 'similarity').\\\n",
    "    orderBy('similarity', ascending=False).\\\n",
    "    filter(f.col('id') != example_movie_id)\n",
    "\n",
    "similarities_HTF = results_HTF.withColumn(\n",
    "    'similarity', udf_similiarity_HTF('features')\n",
    ").\\\n",
    "    select('id', 'title', 'similarity').\\\n",
    "    orderBy('similarity', ascending=False).\\\n",
    "    filter(f.col('id') != example_movie_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer Method\n",
      "+------+---------------------+----------+\n",
      "|id    |title                |similarity|\n",
      "+------+---------------------+----------+\n",
      "|966   |The Magnificent Seven|0.43275064|\n",
      "|149218|Come Dance with Me   |0.4299698 |\n",
      "|206183|Bad Karma            |0.42188314|\n",
      "|52505 |The Other Woman      |0.4023576 |\n",
      "|36056 |The Ring             |0.4016651 |\n",
      "+------+---------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Time elapsed: 85.0 seconds.\n",
      "\n",
      "HashingTF Method\n",
      "+------+--------------------+----------+\n",
      "|id    |title               |similarity|\n",
      "+------+--------------------+----------+\n",
      "|353069|Mother's Day        |0.38411635|\n",
      "|157129|Table No. 21        |0.3616478 |\n",
      "|37602 |Oysters at Nam Kee's|0.35061252|\n",
      "|16520 |The King and I      |0.33171204|\n",
      "|361745|Autumn Dreams       |0.3298108 |\n",
      "+------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Time elapsed: 95.0 seconds.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Show similarity values\n",
    "start = time.time()\n",
    "print('--- CountVectorizer Method ---')\n",
    "similarities_CV.show(truncate=False, n=5)\n",
    "end = time.time()\n",
    "print(f'Time elapsed: {np.round(end-start)} seconds.\\n\\n')\n",
    "\n",
    "start = time.time()\n",
    "print('--- HashingTF Method ---')\n",
    "similarities_HTF.show(truncate=False, n=5)\n",
    "end = time.time()\n",
    "print(f'Time elapsed: {np.round(end-start)} seconds.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results were similar in time and in value spread, although for *HashingTF* the first recommendation is slightly clearer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "+ [SVD](https://machinelearningmastery.com/using-singular-value-decomposition-to-build-a-recommender-system/)\n",
    "\n",
    "+ [TF-IDF and Word2Vec Documentation (PySpark)](https://spark.apache.org/docs/3.5.2/mllib-feature-extraction.html#tf-idf)\n",
    "\n",
    "+ [TF-IDF with CountVectorize](https://www.analyticsvidhya.com/blog/2022/09/implementing-count-vectorizer-and-tf-idf-in-nlp-using-pyspark/)\n",
    "\n",
    "+ [TF-IDF Example](https://runawayhorse001.github.io/LearningApacheSpark/manipulation.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
